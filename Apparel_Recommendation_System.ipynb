{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary packages.\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "#plotly.offline.init_notebook_mode(connected=True)\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# we have give a json file which consists of all information about\n",
    "# the products\n",
    "# loading the data using pandas' read_json file.\n",
    "data = pd.read_json('tops_fashion.json')\n",
    "\n",
    "#print ('Number of data points : ', data.shape[0] ,'\\n'\n",
    "#       'Number of features/variables:', data.shape[1])\n",
    "\n",
    "print(\"These are all the features in the dataset: \\n\")\n",
    "print(data.columns)\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "data = data[['asin', 'brand', 'color', 'medium_image_url', 'product_type_name', 'title', 'formatted_price']]\n",
    "\n",
    "print ('Number of data points : ', data.shape[0], '\\n'\n",
    "      'Number of features:', data.shape[1])\n",
    "print(\"These are all the important 7 features in the dataset: \\n\")\n",
    "print(data.columns)\n",
    "#print(data.head()) # prints the top rows in the table.\n",
    "\n",
    "# We have total 72 unique type of product_type_names\n",
    "#print(data['product_type_name'].describe())\n",
    "# 91.62% (167794/183138) of the products are shirts,\n",
    "# names of different product types\n",
    "print(\"Names of different product types:\\n\")\n",
    "print(data['product_type_name'].unique())\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "# find the 10 most frequent product_type_names.\n",
    "print(\"These are the most freequently occuring product type names: \\n\")\n",
    "product_type_count = Counter(list(data['product_type_name']))\n",
    "print(product_type_count.most_common(10))\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "#BASIC STATS FOR BRANDS\n",
    "# there are 10577 unique brands\n",
    "print(\"BASIC STATS FOR BRANDS: \\n\")\n",
    "print(data['brand'].describe())\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "# 183138 - 182987 = 151 missing values.\n",
    "print(\"These are the most freequently occuring brand names: \\n\")\n",
    "brand_count = Counter(list(data['brand']))\n",
    "print(brand_count.most_common(10))\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "#BASIC STATS FOR COLOR\n",
    "print(\"BASIC STATS FOR COLOR: \\n\")\n",
    "print(data['color'].describe())\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "# we have 7380 unique colors\n",
    "# 7.2% of products are black in color\n",
    "# 64956 of 183138 products have brand information. That's approx 35.4%.\n",
    "#color_count = Counter(list(data['color']))\n",
    "#color_count.most_common(10)\n",
    "\n",
    "#Basic stats for the feature: formatted_price\n",
    "print(\"BASIC STATS FOR PRICE: \\n\")\n",
    "print(data['formatted_price'].describe())\n",
    "# Only 28,395 (15.5% of whole data) products with price information\n",
    "#price_count = Counter(list(data['formatted_price']))\n",
    "#price_count.most_common(10)\n",
    " #BASIC STATS FOR TITLE\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print(\"BASIC STATS FOR TITLE: \\n\")\n",
    "print(data['title'].describe())\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "# All of the products have a title. \n",
    "# Titles are fairly descriptive of what the product is. \n",
    "price_count = Counter(list(data['title']))\n",
    "price_count.most_common(10)\n",
    "#data.to_pickle('pickels/180k_apparel_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "\n",
    "data = pd.read_pickle('pickels/180k_apparel_data')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning part 1\n",
    "\n",
    "data = pd.read_pickle('pickels/180k_apparel_data')\n",
    "print(data.shape[1])\n",
    "print('Number of data points before eliminating price=NULL :', data.shape[0])\n",
    "\n",
    "# consider products which have price information\n",
    "#data['formatted_price'].isnull() #=> gives the information \n",
    "#about the dataframe row's which have null values price == None|Null\n",
    "data = data.loc[~data['formatted_price'].isnull()]\n",
    "print('Number of data points After eliminating price=NULL :', data.shape[0])\n",
    "\n",
    "# consider products which have color information\n",
    "#data['color'].isnull() # => gives the information about the dataframe row's which have null values price == None|Null\n",
    "data =data.loc[~data['color'].isnull()]\n",
    "print('Number of data points After eliminating color=NULL :', data.shape[0])\n",
    "\n",
    "data.to_pickle('pickels/28k_apparel_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning part 2\n",
    "\n",
    "# read data from pickle file from previous stage\n",
    "data = pd.read_pickle('pickels/28k_apparel_data')\n",
    "\n",
    "# find number of products that have duplicate titles.\n",
    "print(sum(data.duplicated('title')))\n",
    "# we have 2325 products which have same title but different color\n",
    "data = pd.read_pickle('pickels/28k_apparel_data')\n",
    "\n",
    "# Remove All products with very few words in title\n",
    "data_sorted = data[data['title'].apply(lambda x: len(x.split())>4)]\n",
    "print(\"After removal of products with short description:\", data_sorted.shape[0])\n",
    "# Sort the whole data based on title (alphabetical order of title) \n",
    "data_sorted.sort_values('title',inplace=True, ascending=False)\n",
    "print(data_sorted.head())\n",
    "\n",
    "indices = []\n",
    "for i,row in data_sorted.iterrows():\n",
    "    indices.append(i)\n",
    "\n",
    "import itertools\n",
    "stage1_dedupe_asins = []\n",
    "i = 0\n",
    "j = 0\n",
    "num_data_points = data_sorted.shape[0]\n",
    "while i < num_data_points and j < num_data_points:\n",
    "    \n",
    "    previous_i = i\n",
    "\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    a = data['title'].loc[indices[i]].split()\n",
    "\n",
    "    # search for the similar products sequentially \n",
    "    j = i+1\n",
    "    while j < num_data_points:\n",
    "\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'Small']\n",
    "        b = data['title'].loc[indices[j]].split()\n",
    "\n",
    "        # store the maximum length of two strings\n",
    "        length = max(len(a), len(b))\n",
    "\n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count  = 0\n",
    "\n",
    "        # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "        # example: a =['a', 'b', 'c', 'd']\n",
    "        # b = ['a', 'b', 'd']\n",
    "        # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0] == k[1]):\n",
    "                count += 1\n",
    "\n",
    "        # if the number of words in which both strings differ are > 2 , we are considering it as those two apperals are different\n",
    "        # if the number of words in which both strings differ are < 2 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) > 2: # number of words in which both sensences differ\n",
    "            # if both strings are differ by more than 2 words we include the 1st string index\n",
    "            stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[i]])\n",
    "\n",
    "            # if the comaprision between is between num_data_points, num_data_points-1 strings and they differ in more than 2 words we include both\n",
    "            if j == num_data_points-1: stage1_dedupe_asins.append(data_sorted['asin'].loc[indices[j]])\n",
    "\n",
    "            # start searching for similar apperals corresponds 2nd string\n",
    "            i = j\n",
    "            break\n",
    "        else:\n",
    "            j += 1\n",
    "    if previous_i == i:\n",
    "        break\n",
    "data = data.loc[data['asin'].isin(stage1_dedupe_asins)]\n",
    "print('Number of data points : ', data.shape[0])\n",
    "#data.to_pickle('pickels/17k_apperal_data')\n",
    "#loading 17k data\n",
    "data = pd.read_pickle('pickels/17k_apperal_data')\n",
    "\n",
    "# This code snippet takes significant amount of time.\n",
    "# O(n^2) time.\n",
    "# Takes about an hour to run on a decent computer.\n",
    "\n",
    "indices = []\n",
    "for i,row in data.iterrows():\n",
    "    indices.append(i)\n",
    "\n",
    "stage2_dedupe_asins = []\n",
    "while len(indices)!=0:\n",
    "    i = indices.pop()\n",
    "    stage2_dedupe_asins.append(data['asin'].loc[i])\n",
    "    # consider the first apperal's title\n",
    "    a = data['title'].loc[i].split()\n",
    "    # store the list of words of ith string in a, ex: a = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "    for j in indices:\n",
    "        \n",
    "        b = data['title'].loc[j].split()\n",
    "        # store the list of words of jth string in b, ex: b = ['tokidoki', 'The', 'Queen', 'of', 'Diamonds', 'Women's', 'Shirt', 'X-Large']\n",
    "        \n",
    "        length = max(len(a),len(b))\n",
    "        \n",
    "        # count is used to store the number of words that are matched in both strings\n",
    "        count  = 0\n",
    "\n",
    "        # itertools.zip_longest(a,b): will map the corresponding words in both strings, it will appened None in case of unequal strings\n",
    "        # example: a =['a', 'b', 'c', 'd']\n",
    "        # b = ['a', 'b', 'd']\n",
    "        # itertools.zip_longest(a,b): will give [('a','a'), ('b','b'), ('c','d'), ('d', None)]\n",
    "        for k in itertools.zip_longest(a,b): \n",
    "            if (k[0]==k[1]):\n",
    "                count += 1\n",
    "\n",
    "        # if the number of words in which both strings differ are < 3 , we are considering it as those two apperals are same, hence we are ignoring them\n",
    "        if (length - count) < 3:\n",
    "            indices.remove(j)\n",
    "            \n",
    "# from whole previous products we will consider only \n",
    "# the products that are found in previous cell \n",
    "data = data.loc[data['asin'].isin(stage2_dedupe_asins)]\n",
    "\n",
    "print('Number of data points after stage two of dedupe: ',data.shape[0])\n",
    "# from 17k apperals we reduced to 16k apperals\n",
    "#data.to_pickle('pickels/16k_apparel_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "\n",
    "data = pd.read_pickle('pickels/16k_apperal_data')\n",
    "\n",
    "\n",
    "\n",
    "# we use the list of stop words that are downloaded from nltk lib.\n",
    "print(\"running\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print ('list of stop words:', stop_words)\n",
    "\n",
    "def nlp_preprocessing(total_text, index, column):\n",
    "    if type(total_text) is not int:\n",
    "        string = \"\"\n",
    "        for words in total_text.split():\n",
    "            # remove the special chars in review like '\"#$@!%^&*()_+-~?>< etc.\n",
    "            word = (\"\".join(e for e in words if e.isalnum()))\n",
    "            # Conver all letters to lower-case\n",
    "            word = word.lower()\n",
    "            # stop-word removal\n",
    "            if not word in stop_words:\n",
    "                string += word + \" \"\n",
    "        data[column][index] = string\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "start_time = time.clock()\n",
    "# we take each title and we text-preprocess it.\n",
    "for index, row in data.iterrows():\n",
    "    nlp_preprocessing(row['title'], index, 'title')\n",
    "# we print the time it took to preprocess whole titles \n",
    "print(time.clock() - start_time, \"seconds\")\n",
    "print()\n",
    "print(data.head())\n",
    "#data.to_pickle('pickels/16k_apperal_data_preprocessed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of words model\n",
    "\n",
    "data = pd.read_pickle('pickels/16k_apperal_data_preprocessed')\n",
    "#print(data.head())\n",
    "\n",
    "\n",
    "#Display an image\n",
    "def display_img(url,ax,fig):\n",
    "    # we get the url of the apparel and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # we will display it\n",
    "    plt.imshow(img)\n",
    "  \n",
    "#plotting code to understand the algorithm's decision.\n",
    "def plot_heatmap(keys, values, labels, url, text):\n",
    "        # keys: list of words of recommended title\n",
    "        # values: len(values) ==  len(keys), values(i) represents the occurence of the word keys(i)\n",
    "        # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "                # if model == 'bag of words': labels(i) = values(i)\n",
    "                # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "                # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "        # url : apparel's url\n",
    "\n",
    "        # we will devide the whole figure into two parts\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[4,1], height_ratios=[4,1]) \n",
    "        fig = plt.figure(figsize=(18,4))\n",
    "        \n",
    "        # 1st, ploting heat map that represents the count of commonly ocurred words in title2\n",
    "        ax = plt.subplot(gs[0])\n",
    "        # it displays a cell in white color if the word is intersection(lis of words of title1 and list of words of title2), in black if not\n",
    "        ax = sns.heatmap(np.array([values]), annot=np.array([labels]))\n",
    "        ax.set_xticklabels(keys) # set that axis labels as the words of title\n",
    "        ax.set_title(text) # apparel title\n",
    "        \n",
    "        # 2nd, plotting image of the the apparel\n",
    "        ax = plt.subplot(gs[1])\n",
    "        # we don't want any grid lines for image and no labels on x-axis and y-axis\n",
    "        ax.grid(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # we call dispaly_img based with paramete url\n",
    "        display_img(url, ax, fig)\n",
    "        \n",
    "        # displays combine figure ( heat map and image together)\n",
    "        plt.show()\n",
    "    \n",
    "def plot_heatmap_image(doc_id, vec1, vec2, url, text, model):\n",
    "\n",
    "    # doc_id : index of the title1\n",
    "    # vec1 : input apparels's vector, it is of a dict type {word:count}\n",
    "    # vec2 : recommended apparels's vector, it is of a dict type {word:count}\n",
    "    # url : apparels image url\n",
    "    # text: title of recomonded apparel (used to keep title of image)\n",
    "    # model, it can be any of the models, \n",
    "        # 1. bag_of_words\n",
    "        # 2. tfidf\n",
    "        # 3. idf\n",
    "\n",
    "    # we find the common words in both titles, because these only words contribute to the distance between two title vec's\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys()) \n",
    "\n",
    "    # we set the values of non intersecting words to zero, this is just to show the difference in heatmap\n",
    "    for i in vec2:\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "\n",
    "    # for labeling heatmap, keys contains list of all words in title2\n",
    "    keys = list(vec2.keys())\n",
    "    #  if ith word in intersection(lis of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "        # if model == 'bag of words': labels(i) = values(i)\n",
    "        # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "        # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "\n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    elif model == 'tfidf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # tfidf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # tfidf_title_features[doc_id, index_of_word_in_corpus] will give the tfidf value of word in given document (doc_id)\n",
    "            if x in  tfidf_title_vectorizer.vocabulary_:\n",
    "                labels.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # idf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # idf_title_features[doc_id, index_of_word_in_corpus] will give the idf value of word in given document (doc_id)\n",
    "            if x in  idf_title_vectorizer.vocabulary_:\n",
    "                labels.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "    plot_heatmap(keys, values, labels, url, text)\n",
    "\n",
    "\n",
    "# this function gets a list of words along with the frequency of each \n",
    "# word given \"text\"\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    # words stores list of all words in given string, you can try 'words = text.split()' this will also gives same result\n",
    "    return Counter(words) # Counter counts the occurence of each word in list, it returns dict type object {word1:count}\n",
    "\n",
    "\n",
    "\n",
    "def get_result(doc_id, content_a, content_b, url, model):\n",
    "    text1 = content_a\n",
    "    text2 = content_b\n",
    "    \n",
    "    # vector1 = dict{word11:#count, word12:#count, etc.}\n",
    "    vector1 = text_to_vector(text1)\n",
    "\n",
    "    # vector1 = dict{word21:#count, word22:#count, etc.}\n",
    "    vector2 = text_to_vector(text2)\n",
    "    plot_heatmap_image(doc_id, vector1, vector2, url, text2, model)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "title_vectorizer = CountVectorizer()\n",
    "title_features   = title_vectorizer.fit_transform(data['title'])\n",
    "print(title_features.get_shape()) # get number of rows and columns in feature matrix.\n",
    "\n",
    "\n",
    "def bag_of_words_model(doc_id, num_results):\n",
    "\n",
    "    pairwise_dist = pairwise_distances(title_features,title_features[doc_id])\n",
    "\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    df_indices = list(data.index[indices])\n",
    "    \n",
    "    for i in range(0,len(indices)):\n",
    "        # we will pass 1. doc_id, 2. title1, 3. title2, url, model\n",
    "        get_result(indices[i],data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], 'bag_of_words')\n",
    "        print('ASIN :',data['asin'].loc[df_indices[i]])\n",
    "        print ('Brand:', data['brand'].loc[df_indices[i]])\n",
    "        print ('Title:', data['title'].loc[df_indices[i]])\n",
    "        print ('Euclidean similarity with the query image :', pdists[i])\n",
    "        print('='*60)\n",
    "\n",
    "#call the bag-of-words model for a product to get similar products.\n",
    "bag_of_words_model(12566, 10) # change the index if you want to.\n",
    "\n",
    "\n",
    "#try  12566 remember\n",
    "#try 12321 remember\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf-idf\n",
    "\n",
    "#import all the necessary packages.\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "data = pd.read_pickle('pickels/16k_apperal_data_preprocessed')\n",
    "#print(data.head())\n",
    "\n",
    "\n",
    "#Display an image\n",
    "def display_img(url,ax,fig):\n",
    "    # we get the url of the apparel and download it\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    # we will display it\n",
    "    plt.imshow(img)\n",
    "  \n",
    "#plotting code to understand the algorithm's decision.\n",
    "def plot_heatmap(keys, values, labels, url, text):\n",
    "        # keys: list of words of recommended title\n",
    "        # values: len(values) ==  len(keys), values(i) represents the occurence of the word keys(i)\n",
    "        # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "                # if model == 'bag of words': labels(i) = values(i)\n",
    "                # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "                # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "        # url : apparel's url\n",
    "\n",
    "        # we will devide the whole figure into two parts\n",
    "        gs = gridspec.GridSpec(2, 2, width_ratios=[4,1], height_ratios=[4,1]) \n",
    "        fig = plt.figure(figsize=(18,4))\n",
    "        \n",
    "        # 1st, ploting heat map that represents the count of commonly ocurred words in title2\n",
    "        ax = plt.subplot(gs[0])\n",
    "        # it displays a cell in white color if the word is intersection(lis of words of title1 and list of words of title2), in black if not\n",
    "        ax = sns.heatmap(np.array([values]), annot=np.array([labels]))\n",
    "        ax.set_xticklabels(keys) # set that axis labels as the words of title\n",
    "        ax.set_title(text) # apparel title\n",
    "        \n",
    "        # 2nd, plotting image of the the apparel\n",
    "        ax = plt.subplot(gs[1])\n",
    "        # we don't want any grid lines for image and no labels on x-axis and y-axis\n",
    "        ax.grid(False)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # we call dispaly_img based with paramete url\n",
    "        display_img(url, ax, fig)\n",
    "        \n",
    "        # displays combine figure ( heat map and image together)\n",
    "        plt.show()\n",
    "    \n",
    "def plot_heatmap_image(doc_id, vec1, vec2, url, text, model):\n",
    "\n",
    "    # doc_id : index of the title1\n",
    "    # vec1 : input apparels's vector, it is of a dict type {word:count}\n",
    "    # vec2 : recommended apparels's vector, it is of a dict type {word:count}\n",
    "    # url : apparels image url\n",
    "    # text: title of recomonded apparel (used to keep title of image)\n",
    "    # model, it can be any of the models, \n",
    "        # 1. bag_of_words\n",
    "        # 2. tfidf\n",
    "        # 3. idf\n",
    "\n",
    "    # we find the common words in both titles, because these only words contribute to the distance between two title vec's\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys()) \n",
    "\n",
    "    # we set the values of non intersecting words to zero, this is just to show the difference in heatmap\n",
    "    for i in vec2:\n",
    "        if i not in intersection:\n",
    "            vec2[i]=0\n",
    "\n",
    "    # for labeling heatmap, keys contains list of all words in title2\n",
    "    keys = list(vec2.keys())\n",
    "    #  if ith word in intersection(lis of words of title1 and list of words of title2): values(i)=count of that word in title2 else values(i)=0 \n",
    "    values = [vec2[x] for x in vec2.keys()]\n",
    "    \n",
    "    # labels: len(labels) == len(keys), the values of labels depends on the model we are using\n",
    "        # if model == 'bag of words': labels(i) = values(i)\n",
    "        # if model == 'tfidf weighted bag of words':labels(i) = tfidf(keys(i))\n",
    "        # if model == 'idf weighted bag of words':labels(i) = idf(keys(i))\n",
    "\n",
    "    if model == 'bag_of_words':\n",
    "        labels = values\n",
    "    elif model == 'tfidf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # tfidf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # tfidf_title_features[doc_id, index_of_word_in_corpus] will give the tfidf value of word in given document (doc_id)\n",
    "            if x in  tfidf_title_vectorizer.vocabulary_:\n",
    "                labels.append(tfidf_title_features[doc_id, tfidf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    elif model == 'idf':\n",
    "        labels = []\n",
    "        for x in vec2.keys():\n",
    "            # idf_title_vectorizer.vocabulary_ it contains all the words in the corpus\n",
    "            # idf_title_features[doc_id, index_of_word_in_corpus] will give the idf value of word in given document (doc_id)\n",
    "            if x in  idf_title_vectorizer.vocabulary_:\n",
    "                labels.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[x]])\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "    plot_heatmap(keys, values, labels, url, text)\n",
    "\n",
    "\n",
    "# this function gets a list of words along with the frequency of each \n",
    "# word given \"text\"\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    # words stores list of all words in given string, you can try 'words = text.split()' this will also gives same result\n",
    "    return Counter(words) # Counter counts the occurence of each word in list, it returns dict type object {word1:count}\n",
    "\n",
    "\n",
    "\n",
    "def get_result(doc_id, content_a, content_b, url, model):\n",
    "    text1 = content_a\n",
    "    text2 = content_b\n",
    "    \n",
    "    # vector1 = dict{word11:#count, word12:#count, etc.}\n",
    "    vector1 = text_to_vector(text1)\n",
    "\n",
    "    # vector1 = dict{word21:#count, word22:#count, etc.}\n",
    "    vector2 = text_to_vector(text2)\n",
    "    plot_heatmap_image(doc_id, vector1, vector2, url, text2, model)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "title_vectorizer = CountVectorizer()\n",
    "title_features   = title_vectorizer.fit_transform(data['title'])\n",
    "print(title_features.get_shape()) # get number of rows and columns in feature matrix.\n",
    "\n",
    "\n",
    "tfidf_title_vectorizer = TfidfVectorizer(min_df = 0)\n",
    "tfidf_title_features = tfidf_title_vectorizer.fit_transform(data['title'])\n",
    "# tfidf_title_features.shape = #data_points * #words_in_corpus\n",
    "# CountVectorizer().fit_transform(courpus) returns the a sparase matrix of dimensions #data_points * #words_in_corpus\n",
    "# tfidf_title_features[doc_id, index_of_word_in_corpus] = tfidf values of the word in given doc\n",
    "\n",
    "\n",
    "def tfidf_model(doc_id, num_results):\n",
    "    # doc_id: apparel's id in given corpus\n",
    "    \n",
    "    # pairwise_dist will store the distance from given input apparel to all remaining apparels\n",
    "    # the metric we used here is cosine, the coside distance is mesured as K(X, Y) = <X, Y> / (||X||*||Y||)\n",
    "    # http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity\n",
    "    pairwise_dist = pairwise_distances(tfidf_title_features,tfidf_title_features[doc_id])\n",
    "\n",
    "    # np.argsort will return indices of 9 smallest distances\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    #pdists will store the 9 smallest distances\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    #data frame indices of the 9 smallest distace's\n",
    "    df_indices = list(data.index[indices])\n",
    "\n",
    "    for i in range(0,len(indices)):\n",
    "        # we will pass 1. doc_id, 2. title1, 3. title2, url, model\n",
    "        get_result(indices[i], data['title'].loc[df_indices[0]], data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], 'tfidf')\n",
    "        print('ASIN :',data['asin'].loc[df_indices[i]])\n",
    "        print('BRAND :',data['brand'].loc[df_indices[i]])\n",
    "        print ('Eucliden distance from the given image :', pdists[i])\n",
    "        print('='*125)\n",
    "tfidf_model(12566, 10)\n",
    "# in the output heat map each value represents the tfidf values of the label word, the color represents the intersection with inputs title\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordtovec\n",
    "\n",
    "\n",
    "#import all the necessary packages.\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from matplotlib import gridspec\n",
    "from scipy.sparse import hstack\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "data = pd.read_pickle('pickels/16k_apperal_data_preprocessed')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "with open('word2vec_model', 'rb') as handle:\n",
    "    model = pickle.load(handle)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def get_word_vec(sentence, doc_id, m_name):\n",
    "    # sentence : title of the apparel\n",
    "    # doc_id: document id in our corpus\n",
    "    # m_name: model information it will take two values\n",
    "        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n",
    "        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n",
    "    vec = []\n",
    "    for i in sentence.split():\n",
    "        if i in vocab:\n",
    "            if m_name == 'weighted' and i in  idf_title_vectorizer.vocabulary_:\n",
    "                vec.append(idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[i]] * model[i])\n",
    "            elif m_name == 'avg':\n",
    "                vec.append(model[i])\n",
    "        else:\n",
    "            # if the word in our courpus is not there in the google word2vec corpus, we are just ignoring it\n",
    "            vec.append(np.zeros(shape=(300,)))\n",
    "    # we will return a numpy array of shape (#number of words in title * 300 ) 300 = len(w2v_model[word])\n",
    "    # each row represents the word2vec representation of each word (weighted/avg) in given sentance \n",
    "    return  np.array(vec)\n",
    "\n",
    "def get_distance(vec1, vec2):\n",
    "    # vec1 = np.array(#number_of_words_title1 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    # vec2 = np.array(#number_of_words_title2 * 300), each row is a vector of length 300 corresponds to each word in give title\n",
    "    \n",
    "    final_dist = []\n",
    "    # for each vector in vec1 we caluclate the distance(euclidean) to all vectors in vec2\n",
    "    for i in vec1:\n",
    "        dist = []\n",
    "        for j in vec2:\n",
    "            # np.linalg.norm(i-j) will result the euclidean distance between vectors i, j\n",
    "            dist.append(np.linalg.norm(i-j))\n",
    "        final_dist.append(np.array(dist))\n",
    "    # final_dist = np.array(#number of words in title1 * #number of words in title2)\n",
    "    # final_dist[i,j] = euclidean distance between vectors i, j\n",
    "    return np.array(final_dist)\n",
    "\n",
    "\n",
    "def heat_map_w2v(sentence1, sentence2, url, doc_id1, doc_id2, model):\n",
    "    # sentance1 : title1, input apparel\n",
    "    # sentance2 : title2, recommended apparel\n",
    "    # url: apparel image url\n",
    "    # doc_id1: document id of input apparel\n",
    "    # doc_id2: document id of recommended apparel\n",
    "    # model: it can have two values, 1. avg 2. weighted\n",
    "    \n",
    "    #s1_vec = np.array(#number_of_words_title1 * 300), each row is a vector(weighted/avg) of length 300 corresponds to each word in give title\n",
    "    s1_vec = get_word_vec(sentence1, doc_id1, model)\n",
    "    #s2_vec = np.array(#number_of_words_title1 * 300), each row is a vector(weighted/avg) of length 300 corresponds to each word in give title\n",
    "    s2_vec = get_word_vec(sentence2, doc_id2, model)\n",
    "\n",
    "    # s1_s2_dist = np.array(#number of words in title1 * #number of words in title2)\n",
    "    # s1_s2_dist[i,j] = euclidean distance between words i, j\n",
    "    s1_s2_dist = get_distance(s1_vec, s2_vec)\n",
    "\n",
    "    \n",
    "    \n",
    "    # devide whole figure into 2 parts 1st part displays heatmap 2nd part displays image of apparel\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4,1],height_ratios=[2,1]) \n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    \n",
    "    ax = plt.subplot(gs[0])\n",
    "    # ploting the heap map based on the pairwise distances\n",
    "    ax = sns.heatmap(np.round(s1_s2_dist,4), annot=True)\n",
    "    # set the x axis labels as recommended apparels title\n",
    "    ax.set_xticklabels(sentence2.split())\n",
    "    # set the y axis labels as input apparels title\n",
    "    ax.set_yticklabels(sentence1.split())\n",
    "    # set title as recommended apparels title\n",
    "    ax.set_title(sentence2)\n",
    "    \n",
    "    ax = plt.subplot(gs[1])\n",
    "    # we remove all grids and axis labels for image\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    display_img(url, ax, fig)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "# vocab = stores all the words that are there in google w2v model\n",
    "# vocab = model.wv.vocab.keys() # if you are using Google word2Vec\n",
    "\n",
    "vocab = model.keys()\n",
    "# this function will add the vectors of each word and returns the avg vector of given sentance\n",
    "def build_avg_vec(sentence, num_features, doc_id, m_name):\n",
    "    # sentace: its title of the apparel\n",
    "    # num_features: the lenght of word2vec vector, its values = 300\n",
    "    # m_name: model information it will take two values\n",
    "        # if  m_name == 'avg', we will append the model[i], w2v representation of word i\n",
    "        # if m_name == 'weighted', we will multiply each w2v[word] with the idf(word)\n",
    "\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    # we will intialize a vector of size 300 with all zeros\n",
    "    # we add each word2vec(wordi) to this fetureVec\n",
    "    nwords = 0\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        nwords += 1\n",
    "        if word in vocab:\n",
    "            if m_name == 'weighted' and word in  idf_title_vectorizer.vocabulary_:\n",
    "                featureVec = np.add(featureVec, idf_title_features[doc_id, idf_title_vectorizer.vocabulary_[word]] * model[word])\n",
    "            elif m_name == 'avg':\n",
    "                featureVec = np.add(featureVec, model[word])\n",
    "    if(nwords>0):\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    # returns the avg vector of given sentance, its of shape (1, 300)\n",
    "    return featureVec\n",
    "\n",
    "doc_id = 0\n",
    "w2v_title = []\n",
    "# for every title we build a avg vector representation\n",
    "for i in data['title']:\n",
    "    w2v_title.append(build_avg_vec(i, 300, doc_id,'avg'))\n",
    "    doc_id += 1\n",
    "\n",
    "# w2v_title = np.array(# number of doc in courpus * 300), each row corresponds to a doc \n",
    "w2v_title = np.array(w2v_title)\n",
    "\n",
    "\n",
    "def avg_w2v_model(doc_id, num_results):\n",
    "    # doc_id: apparel's id in given corpus\n",
    "    \n",
    "    # dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
    "    pairwise_dist = pairwise_distances(w2v_title, w2v_title[doc_id].reshape(1,-1))\n",
    "\n",
    "    # np.argsort will return indices of 9 smallest distances\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    #pdists will store the 9 smallest distances\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    #data frame indices of the 9 smallest distace's\n",
    "    df_indices = list(data.index[indices])\n",
    "    \n",
    "    for i in range(0, len(indices)):\n",
    "        heat_map_w2v(data['title'].loc[df_indices[0]],data['title'].loc[df_indices[i]], data['medium_image_url'].loc[df_indices[i]], indices[0], indices[i], 'avg')\n",
    "        print('ASIN :',data['asin'].loc[df_indices[i]])\n",
    "        print('BRAND :',data['brand'].loc[df_indices[i]])\n",
    "        print ('euclidean distance from given input image :', pdists[i])\n",
    "        print('='*125)\n",
    "\n",
    "        \n",
    "avg_w2v_model(12566, 10)\n",
    "# in the give heat map, each cell contains the euclidean distance between words i, j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('pickels/16k_apperal_data_preprocessed')\n",
    "df_asins = list(data['asin'])\n",
    "asins = list(asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image, SVG, Math, YouTubeVideo\n",
    "\n",
    "def get_similar_products_cnn(doc_id, num_results):\n",
    "    doc_id = asins.index(df_asins[doc_id])\n",
    "    pairwise_dist = pairwise_distances(bottleneck_features_train, bottleneck_features_train[doc_id].reshape(1,-1))\n",
    "\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        rows = data[['medium_image_url','title']].loc[data['asin']==asins[indices[i]]]\n",
    "        for indx, row in rows.iterrows():\n",
    "            display(Image(url=row['medium_image_url'], embed=True))\n",
    "            print('Product Title: ', row['title'])\n",
    "            print('Euclidean Distance from input image:', pdists[i])\n",
    "            print('Amazon Url: www.amzon.com/dp/'+ asins[indices[i]])\n",
    "\n",
    "get_similar_products_cnn(12566, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
